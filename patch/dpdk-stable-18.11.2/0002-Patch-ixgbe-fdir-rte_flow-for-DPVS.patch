From 09a4a420427dda9084669512e7c9c95ebe8586f4 Mon Sep 17 00:00:00 2001
From: wencyu <yuwenchao@qiyi.com>
Date: Tue, 29 Sep 2020 14:45:08 +0800
Subject: [PATCH 2/2] Patch ixgbe fdir rte_flow for DPVS. 1. Ignore fdir flow
 rule priority attribute. 2. Use different fdir soft-id for flow rules
 configured for the same queue. 3. Disable fdir mask settings by rte_flow. 4.
 Allow IPv6 to pass flow rule ETH item validation.

---
 drivers/net/ixgbe/ixgbe_flow.c | 62 ++++++++++++++++++++++++++++++++++--------
 1 file changed, 51 insertions(+), 11 deletions(-)

diff --git a/drivers/net/ixgbe/ixgbe_flow.c b/drivers/net/ixgbe/ixgbe_flow.c
index f0fafeb..05dd5df 100644
--- a/drivers/net/ixgbe/ixgbe_flow.c
+++ b/drivers/net/ixgbe/ixgbe_flow.c
@@ -1428,11 +1428,8 @@ const struct rte_flow_action *next_no_void_action(
 
 	/* not supported */
 	if (attr->priority) {
-		memset(rule, 0, sizeof(struct ixgbe_fdir_rule));
-		rte_flow_error_set(error, EINVAL,
-			RTE_FLOW_ERROR_TYPE_ATTR_PRIORITY,
-			attr, "Not support priority.");
-		return -rte_errno;
+		PMD_DRV_LOG(WARNING, "Ixgbe fdir not support flow priority %d (only 0 is supported), "
+				"ignore and continue....\n", attr->priority);
 	}
 
 	/* check if the first not void action is QUEUE or DROP. */
@@ -1651,7 +1648,7 @@ static inline uint8_t signature_match(const struct rte_flow_item pattern[])
 	 * value. So, we need not do anything for the not provided fields later.
 	 */
 	memset(rule, 0, sizeof(struct ixgbe_fdir_rule));
-	memset(&rule->mask, 0xFF, sizeof(struct ixgbe_hw_fdir_mask));
+	memset(&rule->mask, 0, sizeof(struct ixgbe_hw_fdir_mask)); /* mask default zero */
 	rule->mask.vlan_tci_mask = 0;
 	rule->mask.flex_bytes_mask = 0;
 
@@ -1769,6 +1766,8 @@ static inline uint8_t signature_match(const struct rte_flow_item pattern[])
 			}
 		} else {
 			if (item->type != RTE_FLOW_ITEM_TYPE_IPV4 &&
+					/* Signature mode supports IPv6. */
+					item->type != RTE_FLOW_ITEM_TYPE_IPV6 &&
 					item->type != RTE_FLOW_ITEM_TYPE_VLAN) {
 				memset(rule, 0, sizeof(struct ixgbe_fdir_rule));
 				rte_flow_error_set(error, EINVAL,
@@ -1897,6 +1896,9 @@ static inline uint8_t signature_match(const struct rte_flow_item pattern[])
 		rule->ixgbe_fdir.formatted.flow_type =
 			IXGBE_ATR_FLOW_TYPE_IPV6;
 
+		/* Update flow rule mode by global param. */
+		rule->mode = dev->data->dev_conf.fdir_conf.mode;
+
 		/**
 		 * 1. must signature match
 		 * 2. not support last
@@ -2757,12 +2759,45 @@ static inline uint8_t signature_match(const struct rte_flow_item pattern[])
 	return ixgbe_parse_fdir_act_attr(attr, actions, rule, error);
 }
 
+static inline int
+ixgbe_fdir_rule_patch(struct rte_eth_dev *dev, struct ixgbe_fdir_rule *rule)
+{
+	static uint32_t softid[IXGBE_MAX_RX_QUEUE_NUM] = { 0 };
+
+	if (!rule)
+		return 0;
+
+	if (!dev || !dev->data)
+		return -EINVAL;
+	if (rule->queue >= IXGBE_MAX_RX_QUEUE_NUM)
+		return -EINVAL;
+
+	/* Soft-id for different rx-queue should be different. */
+	rule->soft_id = softid[rule->queue]++;
+
+	/* Disable mask config from rte_flow.
+	 * FIXME:
+	 *   Ixgbe only supports one global mask, all the masks should be the same.
+	 *   Generally, fdir masks should be configured globally before port start.
+	 *   But the rte_flow configures masks at flow creation. So we disable fdir
+	 *   mask configs in rte_flow and configure it globally when port start.
+	 *   Refer to `ixgbe_dev_start/ixgbe_fdir_configure` for details. The global
+	 *   masks are configured into device initially with user specified params.
+	 */
+	rule->b_mask = 0;
+
+	/* Use user-defined mode. */
+	rule->mode = dev->data->dev_conf.fdir_conf.mode;
+
+	return 0;
+}
+
 static int
 ixgbe_parse_fdir_filter(struct rte_eth_dev *dev,
 			const struct rte_flow_attr *attr,
 			const struct rte_flow_item pattern[],
 			const struct rte_flow_action actions[],
-			struct ixgbe_fdir_rule *rule,
+			struct ixgbe_fdir_rule *rule, bool b_patch,
 			struct rte_flow_error *error)
 {
 	int ret;
@@ -2796,13 +2831,18 @@ static inline uint8_t signature_match(const struct rte_flow_item pattern[])
 		rule->ixgbe_fdir.formatted.dst_port != 0))
 		return -ENOTSUP;
 
-	if (fdir_mode == RTE_FDIR_MODE_NONE ||
-	    fdir_mode != rule->mode)
+	if (fdir_mode == RTE_FDIR_MODE_NONE)
 		return -ENOTSUP;
 
 	if (rule->queue >= dev->data->nb_rx_queues)
 		return -ENOTSUP;
 
+	if (ret)
+		return ret;
+
+	if (b_patch)
+		return ixgbe_fdir_rule_patch(dev, rule);
+
 	return ret;
 }
 
@@ -3137,7 +3177,7 @@ static inline uint8_t signature_match(const struct rte_flow_item pattern[])
 
 	memset(&fdir_rule, 0, sizeof(struct ixgbe_fdir_rule));
 	ret = ixgbe_parse_fdir_filter(dev, attr, pattern,
-				actions, &fdir_rule, error);
+				actions, &fdir_rule, true, error);
 	if (!ret) {
 		/* A mask cannot be deleted. */
 		if (fdir_rule.b_mask) {
@@ -3307,7 +3347,7 @@ static inline uint8_t signature_match(const struct rte_flow_item pattern[])
 
 	memset(&fdir_rule, 0, sizeof(struct ixgbe_fdir_rule));
 	ret = ixgbe_parse_fdir_filter(dev, attr, pattern,
-				actions, &fdir_rule, error);
+				actions, &fdir_rule, false, error);
 	if (!ret)
 		return 0;
 
-- 
1.8.3.1

